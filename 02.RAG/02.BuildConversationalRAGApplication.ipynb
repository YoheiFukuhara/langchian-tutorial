{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational RAG\n",
    "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\n",
    "\n",
    "In this guide we focus on adding logic for incorporating historical messages. Further details on chat history management is covered here.\n",
    "\n",
    "We will cover two approaches:\n",
    "\n",
    "Chains, in which we always execute a retrieval step;\n",
    "Agents, in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).\n",
    "For the external knowledge source, we will use the same LLM Powered Autonomous Agents blog post by Lilian Weng from the RAG tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Let's first revisit the Q&A app we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the RAG tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a method of breaking down complex tasks into smaller, more manageable steps. This approach helps agents or models tackle difficult problems by dividing them into simpler subtasks. Techniques like Chain of Thought and Tree of Thoughts are used to guide the decomposition process and enhance performance on complex tasks.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding chat history\n",
    "The chain we have built uses the input query directly to retrieve relevant context. But in a conversational setting, the user query might require conversational context to be understood. For example, consider this exchange:\n",
    "\n",
    "Human: \"What is Task Decomposition?\"\n",
    "\n",
    "AI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\n",
    "\n",
    "Human: \"What are common ways of doing it?\"\n",
    "\n",
    "In order to answer the second question, our system needs to understand that \"it\" refers to \"Task Decomposition.\"\n",
    "\n",
    "We'll need to update two things about our existing app:\n",
    "\n",
    "Prompt: Update our prompt to support historical messages as an input.\n",
    "Contextualizing questions: Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This can be thought of simply as building a new \"history aware\" retriever. Whereas before we had:\n",
    "query -> retriever\n",
    "Now we will have:\n",
    "(query, conversation history) -> LLM -> rephrased query -> retriever\n",
    "Contextualizing the question\n",
    "First we'll need to define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information.\n",
    "\n",
    "We'll use a prompt that includes a MessagesPlaceholder variable under the name \"chat_history\". This allows us to pass in a list of Messages to the prompt using the \"chat_history\" input key, and these messages will be inserted after the system message and before the human message containing the latest question.\n",
    "\n",
    "Note that we leverage a helper function create_history_aware_retriever for this step, which manages the case where chat_history is empty, and otherwise applies prompt | llm | StrOutputParser() | retriever in sequence.\n",
    "\n",
    "create_history_aware_retriever constructs a chain that accepts keys input and chat_history as input, and has the same output schema as a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1つ目のPrompt: \"What is Task Decomposition?\"\n",
    "- 2つ目のPrompt: \"What are common ways of doing it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be achieved through various methods, such as using prompting with language models like LLMs, providing task-specific instructions tailored to the nature of the task (e.g., writing a novel), or incorporating human inputs to guide the decomposition process. These approaches help break down complex tasks into smaller steps, enabling agents to plan and execute tasks more effectively.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stateful management of chat history\n",
    "Here we've gone over how to add application logic for incorporating historical outputs, but we're still manually updating the chat history and inserting it into each input. In a real Q&A application we'll want some way of persisting chat history and some way of automatically inserting and updating it.\n",
    "\n",
    "For this we can use:\n",
    "\n",
    "BaseChatMessageHistory: Store chat history.\n",
    "RunnableWithMessageHistory: Wrapper for an LCEL chain and a BaseChatMessageHistory that handles injecting chat history into inputs and updating it after each invocation.\n",
    "For a detailed walkthrough of how to use these classes together to create a stateful conversational chain, head to the How to add message history (memory) LCEL page.\n",
    "\n",
    "Below, we implement a simple example of the second option, in which chat histories are stored in a simple dict. LangChain manages memory integrations with Redis and other technologies to provide for more robust persistence.\n",
    "\n",
    "Instances of RunnableWithMessageHistory manage the chat history for you. They accept a config with a key (\"session_id\" by default) that specifies what conversation history to fetch and prepend to the input, and append the output to the same conversation history. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition involves breaking down complex tasks into smaller and simpler steps to facilitate problem-solving and planning. Techniques like Chain of Thought and Tree of Thoughts help agents decompose tasks into manageable sub-tasks, enhancing their performance on challenging problems. Task decomposition can be achieved through various methods such as using language models with specific prompts, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be commonly done using techniques such as prompting language models with specific instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", providing task-specific instructions like \"Write a story outline\" for writing a novel, or incorporating human inputs to break down complex tasks into manageable steps. These methods help in organizing thoughts and actions, leading to more efficient problem-solving and planning processes.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation Historyの中身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition involves breaking down complex tasks into smaller and simpler steps to facilitate problem-solving and planning. Techniques like Chain of Thought and Tree of Thoughts help agents decompose tasks into manageable sub-tasks, enhancing their performance on challenging problems. Task decomposition can be achieved through various methods such as using language models with specific prompts, task-specific instructions, or human inputs.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Task decomposition can be commonly done using techniques such as prompting language models with specific instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", providing task-specific instructions like \"Write a story outline\" for writing a novel, or incorporating human inputs to break down complex tasks into manageable steps. These methods help in organizing thoughts and actions, leading to more efficient problem-solving and planning processes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allow you to offload some discretion over the retrieval process. Although their behavior is less predictable than chains, they offer some advantages in this context:\n",
    "\n",
    "Agents generate the input to the retriever directly, without necessarily needing us to explicitly build in contextualization, as we did above;\n",
    "Agents can execute multiple retrieval steps in service of a query, or refrain from executing a retrieval step altogether (e.g., in response to a generic greeting from a user)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval tool\n",
    "Agents can access \"tools\" and manage their execution. In this case, we will convert our retriever into a LangChain tool to be wielded by the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke(\"task decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent constructor\n",
    "Now that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent. Currently we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import chat_agent_executor\n",
    "\n",
    "agent_executor = chat_agent_executor.create_tool_calling_executor(llm, tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraphは可視化ができる  \n",
    "このTutorialだと実験できないが、複雑な分岐とかがかけるのがLnagGraphの魅力っぽい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent_executor.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xjIgEnVDKwL0qR5pKyVEdrxj', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 68, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a4f2d727-b509-45ad-bda7-54a62b87fbb9-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_xjIgEnVDKwL0qR5pKyVEdrxj'}])]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', name='blog_post_retriever', id='4bda8d91-8658-432f-982b-08d76bed9ac6', tool_call_id='call_xjIgEnVDKwL0qR5pKyVEdrxj')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=\"Task decomposition is a technique used in complex tasks where the task is broken down into smaller and simpler steps. This approach helps in transforming big tasks into multiple manageable tasks and provides insights into the model's thinking process. \\n\\nOne method for task decomposition is the Chain of thought (CoT) technique, which instructs the model to think step by step to decompose hard tasks. Another extension called Tree of Thoughts explores multiple reasoning possibilities at each step by creating a tree structure with multiple thoughts per step.\\n\\nTask decomposition can be achieved through various methods such as using simple prompting for LLMs, task-specific instructions, or human inputs. This process helps in planning and executing tasks efficiently by breaking them down into smaller components.\", response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 588, 'total_tokens': 731}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0f3cf044-f927-447f-8f29-d0a5743a70d1-0')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqliteで会話履歴を持ってagent_executorを作るっぽい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "agent_executor = chat_agent_executor.create_tool_calling_executor(\n",
    "    llm, tools, checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Hello Bob! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 67, 'total_tokens': 78}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3f58fddf-0061-4b3c-a3ec-cfc7593cd2b0-0')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi! I'm bob\")]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_v9zxu8ijct1cRxFmhjkaaKOD', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 91, 'total_tokens': 110}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-56b23f77-d146-4411-af3c-28265d0db853-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_v9zxu8ijct1cRxFmhjkaaKOD'}])]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', name='blog_post_retriever', id='e658b089-61af-4d51-bcc5-541074c7806c', tool_call_id='call_v9zxu8ijct1cRxFmhjkaaKOD')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a technique used in complex tasks where the task is broken down into smaller and simpler steps. This approach helps in enhancing model performance by instructing the model to \"think step by step\" and decompose hard tasks into more manageable components. \\n\\nOne method for task decomposition is the Chain of Thought (CoT) technique, which transforms big tasks into multiple manageable tasks by guiding the model to think in smaller steps. Another approach, Tree of Thoughts, extends CoT by exploring multiple reasoning possibilities at each step and creating a tree structure of thoughts.\\n\\nTask decomposition can be achieved through various methods such as using simple prompting for LLMs, task-specific instructions, or human inputs. This process helps in planning and executing complex tasks by breaking them down into smaller and more manageable steps.', response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 611, 'total_tokens': 769}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e9c37b40-f8b4-4d2d-9fd1-d132f6e9518a-0')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jj0ihnufRoKOuMeknt5dYNrV', 'function': {'arguments': '{\"query\":\"Common ways of task decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 792, 'total_tokens': 813}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f1cfb8c9-c778-4ef2-9c51-d0e599b0bbbe-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Common ways of task decomposition'}, 'id': 'call_jj0ihnufRoKOuMeknt5dYNrV'}])]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', name='blog_post_retriever', id='ee7058ee-35a6-4cec-baaf-d05ec27c7881', tool_call_id='call_jj0ihnufRoKOuMeknt5dYNrV')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using LLM with simple prompting: LLMs can be guided with prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ\" to decompose tasks into smaller steps.\\n   \\n2. Task-specific instructions: Providing specific instructions tailored to the task, for example, \"Write a story outline\" for writing a novel, can help in breaking down the task into manageable components.\\n   \\n3. Human inputs: Involving human input in the task decomposition process can also be an effective way to break down complex tasks into smaller and more manageable steps.\\n\\nThese methods of task decomposition help in planning and executing tasks by breaking them down into simpler components for better understanding and implementation.', response_metadata={'token_usage': {'completion_tokens': 148, 'prompt_tokens': 1337, 'total_tokens': 1485}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b1b027ab-4a64-4b20-8791-e4b5cbee193d-0')]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
