{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://smith.langchain.com/hub/welcome/text_summary\n",
    "def build_prompt():\n",
    "    system_message = SystemMessage(\n",
    "        content=\"You are an expert summarizer and analyzer who can help me.\"\n",
    "        )\n",
    "    human_prompt = \"\"\"\\\n",
    "Generate a concise and coherent summary from the given Context. \n",
    "Condense the context into a well-written summary that captures the main ideas, key points, and insights presented in the context. \n",
    "Prioritize clarity and brevity while retaining the essential information. \n",
    "Aim to convey the context's core message and any supporting details that contribute to a comprehensive understanding. \n",
    "Craft the summary to be self-contained, ensuring that readers can grasp the content even if they haven't read the context. \n",
    "Provide context where necessary and avoid excessive technical jargon or verbosity.\n",
    "The goal is to create a summary that effectively communicates the context's content while being easily digestible and engaging.\n",
    "Summary should NOT be more than {word_count} words for {target_audience} audience.\n",
    "CONTEXT: {text}\n",
    "SUMMARY: \n",
    "\"\"\"\n",
    "    human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message, human_message_template]\n",
    "    )\n",
    "    return chat_prompt\n",
    "\n",
    "prompt = build_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an expert summarizer and analyzer who can help me.'),\n",
       " HumanMessage(content=\"Generate a concise and coherent summary from the given Context. \\nCondense the context into a well-written summary that captures the main ideas, key points, and insights presented in the context. \\nPrioritize clarity and brevity while retaining the essential information. \\nAim to convey the context's core message and any supporting details that contribute to a comprehensive understanding. \\nCraft the summary to be self-contained, ensuring that readers can grasp the content even if they haven't read the context. \\nProvide context where necessary and avoid excessive technical jargon or verbosity.\\nThe goal is to create a summary that effectively communicates the context's content while being easily digestible and engaging.\\nSummary should NOT be more than 30 words for General People audience.\\nCONTEXT: ピザドッグは、コッペパンにピザ味の具を挟んで、アルミホイルに包み、オーブンで焼きました。ピザチーズがとろけて、ちょうどいい塩味でおいしく仕上がりました。\\nポトフはフランスの家庭料理です。フランス語で「火にかけた鍋」という意味の「ポット・オー・フー」が料理名となっています。今日はは、鶏肉を使い、たっぷりのキャベツやじゃがいも、玉ねぎなどの野菜を鶏がらスープで煮こみました。\\n\\nSUMMARY: \\n\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\\\n",
    "ピザドッグは、コッペパンにピザ味の具を挟んで、アルミホイルに包み、オーブンで焼きました。ピザチーズがとろけて、ちょうどいい塩味でおいしく仕上がりました。\n",
    "ポトフはフランスの家庭料理です。フランス語で「火にかけた鍋」という意味の「ポット・オー・フー」が料理名となっています。今日はは、鶏肉を使い、たっぷりのキャベツやじゃがいも、玉ねぎなどの野菜を鶏がらスープで煮こみました。\n",
    "\"\"\"\n",
    "prompt.format_prompt(\n",
    "    word_count=\"30\",\n",
    "    target_audience=\"General People\",\n",
    "    text=text\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['target_audience', 'text', 'word_count'] metadata={'lc_hub_owner': 'hardkothari', 'lc_hub_repo': 'text_summary', 'lc_hub_commit_hash': '74594bf038428d6013c0dbbd84968d8fe11e01f48a46b8103682366f97565d4a'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert summarizer and analyzer who can help me.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['target_audience', 'text', 'word_count'], template=\"Generate a concise and coherent summary from the given Context. \\n\\nCondense the context into a well-written summary that captures the main ideas, key points, and insights presented in the context. \\n\\nPrioritize clarity and brevity while retaining the essential information. \\n\\nAim to convey the context's core message and any supporting details that contribute to a comprehensive understanding. \\n\\nCraft the summary to be self-contained, ensuring that readers can grasp the content even if they haven't read the context. \\n\\nProvide context where necessary and avoid excessive technical jargon or verbosity.\\n\\nThe goal is to create a summary that effectively communicates the context's content while being easily digestible and engaging.\\n\\nSummary should NOT be more than {word_count} words for {target_audience} audience.\\n\\nCONTEXT: {text}\\n\\nSUMMARY: \"))]\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hardkothari/text_summary\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
    "This tutorial will show how to build a simple Q&A application over a text data source. Along the way we’ll go over a typical Q&A architecture and highlight additional resources for more advanced Q&A techniques. We’ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "#    {\"context\": text, \"word_count\": 100, \"target_audience\": \"non-technical person\"}\n",
    "     prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Japanese fusion dish, Pizza Dog, features pizza-flavored filling inside a bread bun, wrapped in foil and baked for a delicious, cheesy outcome. On the other hand, Pot-au-feu, a traditional French dish meaning \"pot on the fire,\" consists of chicken, cabbage, potatoes, onions, and vegetables simmered in chicken broth. These dishes combine unique flavors and cultural influences for a satisfying culinary experience.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"text\": text, \n",
    "                  \"word_count\": 100, \n",
    "                  \"target_audience\": \"General People\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
